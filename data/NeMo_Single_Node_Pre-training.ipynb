{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "badee675",
   "metadata": {},
   "source": [
    "# 01. NeMo Framework Single Node Pre-training\n",
    "\n",
    "This notebook demonstrates how to train a GPT-style model with the NVIDIA NeMo Framework.\n",
    "\n",
    "The total training will take a considerable amount of time, but can be achieved on a GPU with as low as 16GB of GPU RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40a08e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A40-16Q\n",
      "__CUDNN VERSION: 8907\n",
      "__Number CUDA Devices: 1\n",
      "__CUDA Device Name: NVIDIA A40-16Q\n",
      "__CUDA Device Total Memory [GB]: 16.966156288\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if not use_cuda:\n",
    "    exit()\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)\n",
    "print('Memory Usage:')\n",
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26c452b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 21 12:15:19 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40-16Q                 On  |   00000000:03:00.0 Off |                    0 |\n",
      "| N/A   N/A    P8             N/A /  N/A  |       0MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1a7a1d",
   "metadata": {},
   "source": [
    "Reinstall tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15cf306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tb-nightly 2.17.0a20240309\n",
      "Uninstalling tb-nightly-2.17.0a20240309:\n",
      "  Successfully uninstalled tb-nightly-2.17.0a20240309\n",
      "\u001b[33mWARNING: Skipping tensorboardX as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: tensorboard 2.9.0\n",
      "Uninstalling tensorboard-2.9.0:\n",
      "  Successfully uninstalled tensorboard-2.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.60.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.24.4)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (4.24.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.4)\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tensorboard\n",
      "Successfully installed tensorboard-2.16.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall tb-nightly tensorboardX tensorboard -y\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aae4758",
   "metadata": {},
   "source": [
    "## Prepare dataset and dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad5d22c",
   "metadata": {},
   "source": [
    "Get tokenizer files\n",
    "This step uses This playbook demonstrates how to train a GPT-style model with the NVIDIA NeMo Framework.\n",
    "\n",
    "The total training will take a considerable amount of time, but can be achieved on a GPU with as low as 16GB of GPU RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53e5aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-21 12:15:28--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.136.224, 16.182.98.208, 16.182.32.152, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.136.224|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1042301 (1018K) [application/json]\n",
      "Saving to: ‘gpt2-vocab.json.1’\n",
      "\n",
      "gpt2-vocab.json.1   100%[===================>]   1018K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-05-21 12:15:28 (7.99 MB/s) - ‘gpt2-vocab.json.1’ saved [1042301/1042301]\n",
      "\n",
      "--2024-05-21 12:15:28--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.169.152, 52.217.236.88, 54.231.136.224, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.169.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 456318 (446K) [text/plain]\n",
      "Saving to: ‘gpt2-merges.txt.1’\n",
      "\n",
      "gpt2-merges.txt.1   100%[===================>] 445.62K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-05-21 12:15:28 (4.32 MB/s) - ‘gpt2-merges.txt.1’ saved [456318/456318]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa2861",
   "metadata": {},
   "source": [
    "Download dataset from official repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2404030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-21 12:15:29--  https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\n",
      "Resolving huggingface.co (huggingface.co)... 18.172.134.4, 18.172.134.24, 18.172.134.124, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.172.134.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1716552929&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjU1MjkyOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=mb4El-WH-XhSuKGesaifCeKiM1UGqzsPHR-gXkw8n7y3MFmIPzr4hnPDdq3D%7E3qFw4kSX%7ExWJ%7ElZs5orLrwytIX0YF8coBbZI8yKprkq0bM-qdK4OuFszTZhRdrGV57tHo1OfUuK9JvgMYuBS8HId0QUJaMCad9efJ%7EJRj%7E-7z9tZbKbkJEFHJKHhIGuIu0dS%7El3uD3nhyQKvD2q7VuYJstA3gBNqfQqh67KzjQPKyN27zuLCNrC2WubQveap5CXU2Xwv5Vh3ADRfKSQe70MM0FB6Rlqf1rHsCaaCUmLvDIqi6QqB8g8grps9eF9JoDunLEDGqmofqpM4AvUr3YRlw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2024-05-21 12:15:29--  https://cdn-lfs.huggingface.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1716552929&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjU1MjkyOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=mb4El-WH-XhSuKGesaifCeKiM1UGqzsPHR-gXkw8n7y3MFmIPzr4hnPDdq3D%7E3qFw4kSX%7ExWJ%7ElZs5orLrwytIX0YF8coBbZI8yKprkq0bM-qdK4OuFszTZhRdrGV57tHo1OfUuK9JvgMYuBS8HId0QUJaMCad9efJ%7EJRj%7E-7z9tZbKbkJEFHJKHhIGuIu0dS%7El3uD3nhyQKvD2q7VuYJstA3gBNqfQqh67KzjQPKyN27zuLCNrC2WubQveap5CXU2Xwv5Vh3ADRfKSQe70MM0FB6Rlqf1rHsCaaCUmLvDIqi6QqB8g8grps9eF9JoDunLEDGqmofqpM4AvUr3YRlw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.154.185.27, 18.154.185.64, 18.154.185.94, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.154.185.27|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13085339 (12M) [text/plain]\n",
      "Saving to: ‘databricks-dolly-15k.jsonl.1’\n",
      "\n",
      "databricks-dolly-15 100%[===================>]  12.48M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-05-21 12:15:29 (107 MB/s) - ‘databricks-dolly-15k.jsonl.1’ saved [13085339/13085339]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37ee128",
   "metadata": {},
   "source": [
    "Convert training data into memory map format\n",
    "\n",
    "The memory map format makes training more efficient, especially with many nodes and GPUs. This step will also tokenize data using the tokenizer model from Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e58b9de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "Vocab size: 50257\n",
      "Output prefix: hfbpe_gpt_training_data\n",
      "Time to startup: 0.2596759796142578\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:52 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:15:53 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "Processing file databricks-dolly-15k.jsonl 1/1\n",
      "Processed 100 documents (50.98372990549123 docs/s, 0.04644264138329231 MB/s).\n",
      "Processed 200 documents (101.92397324021356 docs/s, 0.08213300673368763 MB/s).\n",
      "Processed 300 documents (152.82973217990187 docs/s, 0.1210009901269092 MB/s).\n",
      "Processed 400 documents (203.68787217448482 docs/s, 0.16033648312776627 MB/s).\n",
      "Processed 500 documents (252.72807839232726 docs/s, 0.20201403774401647 MB/s).\n",
      "Processed 600 documents (293.7686889835621 docs/s, 0.23298403650293514 MB/s).\n",
      "Processed 700 documents (327.6579854791006 docs/s, 0.26362246643696485 MB/s).\n",
      "Processed 800 documents (329.86253676301806 docs/s, 0.26681853085163976 MB/s).\n",
      "Processed 900 documents (370.8489414388792 docs/s, 0.3013335486785252 MB/s).\n",
      "Processed 1000 documents (385.55496264919753 docs/s, 0.3209460027770079 MB/s).\n",
      "Processed 1100 documents (423.96528687763845 docs/s, 0.3529479872894984 MB/s).\n",
      "Processed 1200 documents (462.36024004832717 docs/s, 0.3857536344569002 MB/s).\n",
      "Processed 1300 documents (500.7175862536987 docs/s, 0.41575115693358466 MB/s).\n",
      "Processed 1400 documents (539.1008549375161 docs/s, 0.4417030885607006 MB/s).\n",
      "Processed 1500 documents (577.4489619751123 docs/s, 0.47193664693973775 MB/s).\n",
      "Processed 1600 documents (607.8037489745233 docs/s, 0.5020495043747938 MB/s).\n",
      "Processed 1700 documents (645.5626376173369 docs/s, 0.5315628983659751 MB/s).\n",
      "Processed 1800 documents (683.3001552093607 docs/s, 0.562936540468517 MB/s).\n",
      "Processed 1900 documents (688.3069796261701 docs/s, 0.5881021504600861 MB/s).\n",
      "Processed 2000 documents (724.1974577918793 docs/s, 0.6148418984684524 MB/s).\n",
      "Processed 2100 documents (760.0647432889932 docs/s, 0.6480559193969005 MB/s).\n",
      "Processed 2200 documents (795.922516778043 docs/s, 0.6793249898692587 MB/s).\n",
      "Processed 2300 documents (797.9728779030837 docs/s, 0.6818423994129664 MB/s).\n",
      "Processed 2400 documents (831.0105892180691 docs/s, 0.7093877098829482 MB/s).\n",
      "Processed 2500 documents (817.0351759381296 docs/s, 0.6975607988676877 MB/s).\n",
      "Processed 2600 documents (849.3885132587972 docs/s, 0.7218291451149291 MB/s).\n",
      "Processed 2700 documents (880.7327942212662 docs/s, 0.7447346358504858 MB/s).\n",
      "Processed 2800 documents (912.9975723730503 docs/s, 0.7753591860923338 MB/s).\n",
      "Processed 2900 documents (945.2560871910787 docs/s, 0.8006731778448766 MB/s).\n",
      "Processed 3000 documents (938.3515919052571 docs/s, 0.7978524626252611 MB/s).\n",
      "Processed 3100 documents (969.3376501345348 docs/s, 0.8215246037688811 MB/s).\n",
      "Processed 3200 documents (1000.2796835896802 docs/s, 0.8466143290887925 MB/s).\n",
      "Processed 3300 documents (1031.2131824078403 docs/s, 0.8673316519463351 MB/s).\n",
      "Processed 3400 documents (1053.4169154693384 docs/s, 0.8834492508292889 MB/s).\n",
      "Processed 3500 documents (1083.9191984585377 docs/s, 0.9088235954299567 MB/s).\n",
      "Processed 3600 documents (1104.9769388713607 docs/s, 0.9256682736447736 MB/s).\n",
      "Processed 3700 documents (1135.3080918033343 docs/s, 0.9483297341622144 MB/s).\n",
      "Processed 3800 documents (1165.6786167393657 docs/s, 0.9698706804279696 MB/s).\n",
      "Processed 3900 documents (1196.0225841972037 docs/s, 0.9912962223674802 MB/s).\n",
      "Processed 4000 documents (1179.9354064539252 docs/s, 0.978250561687596 MB/s).\n",
      "Processed 4100 documents (1209.0962015163336 docs/s, 0.9994861745229185 MB/s).\n",
      "Processed 4200 documents (1114.5523228027073 docs/s, 0.9261864116063769 MB/s).\n",
      "Processed 4300 documents (1140.6536764191553 docs/s, 0.9473520439546929 MB/s).\n",
      "Processed 4400 documents (1166.8131547421394 docs/s, 0.9666392101631381 MB/s).\n",
      "Processed 4500 documents (1192.9947782055215 docs/s, 0.9893509878453938 MB/s).\n",
      "Processed 4600 documents (1174.4797147811892 docs/s, 0.9782720808632924 MB/s).\n",
      "Processed 4700 documents (1199.683252730582 docs/s, 0.9991389985460103 MB/s).\n",
      "Processed 4800 documents (1224.7831985104028 docs/s, 1.0226371239878393 MB/s).\n",
      "Processed 4900 documents (1249.8037825129545 docs/s, 1.0452945475844966 MB/s).\n",
      "Processed 5000 documents (1275.0561999415595 docs/s, 1.0630165290512386 MB/s).\n",
      "Processed 5100 documents (1300.2701937456318 docs/s, 1.0822433290106386 MB/s).\n",
      "Processed 5200 documents (1325.4821592669366 docs/s, 1.1014633711740438 MB/s).\n",
      "Processed 5300 documents (1350.6366486185325 docs/s, 1.1226198320469367 MB/s).\n",
      "Processed 5400 documents (1375.7809017722716 docs/s, 1.1428234739284737 MB/s).\n",
      "Processed 5500 documents (1400.9526540623467 docs/s, 1.1649550011423249 MB/s).\n",
      "Processed 5600 documents (1417.1074434016223 docs/s, 1.179274893994937 MB/s).\n",
      "Processed 5700 documents (1435.3351009809317 docs/s, 1.1928508973000909 MB/s).\n",
      "Processed 5800 documents (1460.0294034894318 docs/s, 1.2165472890117182 MB/s).\n",
      "Processed 5900 documents (1484.7118747222135 docs/s, 1.232849228725268 MB/s).\n",
      "Processed 6000 documents (1509.507607123395 docs/s, 1.251536795180051 MB/s).\n",
      "Processed 6100 documents (1529.7572586065853 docs/s, 1.2666314380555816 MB/s).\n",
      "Processed 6200 documents (1554.5365159404544 docs/s, 1.282783333779684 MB/s).\n",
      "Processed 6300 documents (1576.74401036503 docs/s, 1.2987738458071139 MB/s).\n",
      "Processed 6400 documents (1596.7766028651097 docs/s, 1.3169920511966986 MB/s).\n",
      "Processed 6500 documents (1615.4214071285044 docs/s, 1.3341137388503057 MB/s).\n",
      "Processed 6600 documents (1638.618889845785 docs/s, 1.351404718303102 MB/s).\n",
      "Processed 6700 documents (1658.4968980462886 docs/s, 1.365844987299459 MB/s).\n",
      "Processed 6800 documents (1679.9214076738958 docs/s, 1.3835839978456494 MB/s).\n",
      "Processed 6900 documents (1683.2179653072362 docs/s, 1.3876535506784125 MB/s).\n",
      "Processed 7000 documents (1707.2682675454485 docs/s, 1.4051921895375847 MB/s).\n",
      "Processed 7100 documents (1731.3302161950728 docs/s, 1.4214292325430722 MB/s).\n",
      "Processed 7200 documents (1730.359025422289 docs/s, 1.4273517383614132 MB/s).\n",
      "Processed 7300 documents (1754.0559022760538 docs/s, 1.4424203142305332 MB/s).\n",
      "Processed 7400 documents (1777.7547040592262 docs/s, 1.460022405614245 MB/s).\n",
      "Processed 7500 documents (1801.3848957250975 docs/s, 1.4803421048969665 MB/s).\n",
      "Processed 7600 documents (1824.9253624107303 docs/s, 1.4973612852052152 MB/s).\n",
      "Processed 7700 documents (1848.458052959901 docs/s, 1.5160923208509922 MB/s).\n",
      "Processed 7800 documents (1872.0126041498045 docs/s, 1.5347365143201743 MB/s).\n",
      "Processed 7900 documents (1855.7334151009704 docs/s, 1.5230127453904194 MB/s).\n",
      "Processed 8000 documents (1878.8669940670923 docs/s, 1.5420462610585834 MB/s).\n",
      "Processed 8100 documents (1902.0178804065165 docs/s, 1.5595071327359016 MB/s).\n",
      "Processed 8200 documents (1925.0041599952358 docs/s, 1.5773873573197195 MB/s).\n",
      "Processed 8300 documents (1941.8344856437645 docs/s, 1.5911614117778305 MB/s).\n",
      "Processed 8400 documents (1906.5625847057572 docs/s, 1.562967564280052 MB/s).\n",
      "Processed 8500 documents (1928.8521605105923 docs/s, 1.5785717238183885 MB/s).\n",
      "Processed 8600 documents (1951.1360138855098 docs/s, 1.596539787071975 MB/s).\n",
      "Processed 8700 documents (1973.4505613936014 docs/s, 1.6148644822762341 MB/s).\n",
      "Processed 8800 documents (1941.9416514799054 docs/s, 1.5896359838848286 MB/s).\n",
      "Processed 8900 documents (1963.071668488927 docs/s, 1.609405207669027 MB/s).\n",
      "Processed 9000 documents (1984.7787870255793 docs/s, 1.6235504123088798 MB/s).\n",
      "Processed 9100 documents (2006.2136425296935 docs/s, 1.6439631776670836 MB/s).\n",
      "Processed 9200 documents (2027.7367309366293 docs/s, 1.6631173546240559 MB/s).\n",
      "Processed 9300 documents (2032.510304533995 docs/s, 1.6666212300078702 MB/s).\n",
      "Processed 9400 documents (2053.824282593273 docs/s, 1.684346405591363 MB/s).\n",
      "Processed 9500 documents (2075.230791040869 docs/s, 1.7012005081897525 MB/s).\n",
      "Processed 9600 documents (2096.59552860345 docs/s, 1.721643463797317 MB/s).\n",
      "Processed 9700 documents (2096.2197224863085 docs/s, 1.72366990598202 MB/s).\n",
      "Processed 9800 documents (2105.021127449399 docs/s, 1.7306567971186164 MB/s).\n",
      "Processed 9900 documents (2102.7917299002333 docs/s, 1.728136310638391 MB/s).\n",
      "Processed 10000 documents (2123.5769420359184 docs/s, 1.7447572614586873 MB/s).\n",
      "Processed 10100 documents (2144.3035363051026 docs/s, 1.7644179552575348 MB/s).\n",
      "Processed 10200 documents (2165.1832244117763 docs/s, 1.7768356033428985 MB/s).\n",
      "Processed 10300 documents (2100.1396015059286 docs/s, 1.7274827126647885 MB/s).\n",
      "Processed 10400 documents (2119.933539640121 docs/s, 1.7472680939146237 MB/s).\n",
      "Processed 10500 documents (2138.299916230864 docs/s, 1.7612522808554385 MB/s).\n",
      "Processed 10600 documents (2158.163694854226 docs/s, 1.7773794951931665 MB/s).\n",
      "Processed 10700 documents (2166.958962831379 docs/s, 1.7873628571628606 MB/s).\n",
      "Processed 10800 documents (2186.7196311435605 docs/s, 1.803158520565604 MB/s).\n",
      "Processed 10900 documents (2206.169991321616 docs/s, 1.8215302677945477 MB/s).\n",
      "Processed 11000 documents (2225.937134160477 docs/s, 1.8397650811218422 MB/s).\n",
      "Processed 11100 documents (2212.4264962321276 docs/s, 1.827175754535563 MB/s).\n",
      "Processed 11200 documents (2231.9314660912146 docs/s, 1.842563711041204 MB/s).\n",
      "Processed 11300 documents (2234.3500923922848 docs/s, 1.8469722404519762 MB/s).\n",
      "Processed 11400 documents (2253.6615811392703 docs/s, 1.8639955397216323 MB/s).\n",
      "Processed 11500 documents (2155.016151285457 docs/s, 1.7852607507433622 MB/s).\n",
      "Processed 11600 documents (2173.1948997065506 docs/s, 1.8006401764643718 MB/s).\n",
      "Processed 11700 documents (2191.4877755098278 docs/s, 1.8152162268144998 MB/s).\n",
      "Processed 11800 documents (2209.7410203463764 docs/s, 1.8299055999470657 MB/s).\n",
      "Processed 11900 documents (2228.0418774697478 docs/s, 1.8430359554091806 MB/s).\n",
      "Processed 12000 documents (2246.4342984338227 docs/s, 1.8553016970043643 MB/s).\n",
      "Processed 12100 documents (2259.210489707634 docs/s, 1.869949608879991 MB/s).\n",
      "Processed 12200 documents (2277.222835765314 docs/s, 1.8847139476976469 MB/s).\n",
      "Processed 12300 documents (2295.3357689993427 docs/s, 1.9005776297798345 MB/s).\n",
      "Processed 12400 documents (2313.422179133106 docs/s, 1.917006252187069 MB/s).\n",
      "Processed 12500 documents (2327.70617575494 docs/s, 1.93116376207306 MB/s).\n",
      "Processed 12600 documents (2343.814165724342 docs/s, 1.9438713771058944 MB/s).\n",
      "Processed 12700 documents (2358.808392979591 docs/s, 1.9591749254786184 MB/s).\n",
      "Processed 12800 documents (2373.759100934575 docs/s, 1.9749589112711605 MB/s).\n",
      "Processed 12900 documents (2385.107824075374 docs/s, 1.9836658761788055 MB/s).\n",
      "Processed 13000 documents (2370.4957834170273 docs/s, 1.9711659709783345 MB/s).\n",
      "Processed 13100 documents (2388.249641439053 docs/s, 1.9878303347451198 MB/s).\n",
      "Processed 13200 documents (2406.0463347031455 docs/s, 2.000107167530446 MB/s).\n",
      "Processed 13300 documents (2423.8542999828155 docs/s, 2.013441417946846 MB/s).\n",
      "Processed 13400 documents (2421.842662218811 docs/s, 2.010632314714861 MB/s).\n",
      "Processed 13500 documents (2398.634403766573 docs/s, 1.9935593715464863 MB/s).\n",
      "Processed 13600 documents (2415.79991695845 docs/s, 2.0098231953582113 MB/s).\n",
      "Processed 13700 documents (2432.93050643193 docs/s, 2.0249573045034013 MB/s).\n",
      "Processed 13800 documents (2449.035032373195 docs/s, 2.039122518528949 MB/s).\n",
      "Processed 13900 documents (2466.3328379444715 docs/s, 2.0530655088913847 MB/s).\n",
      "Processed 14000 documents (2483.602500476036 docs/s, 2.0683288853977246 MB/s).\n",
      "Processed 14100 documents (2486.9086065071565 docs/s, 2.072673456066891 MB/s).\n",
      "Processed 14200 documents (2501.5793364802626 docs/s, 2.0893757492563823 MB/s).\n",
      "Processed 14300 documents (2518.6645077350868 docs/s, 2.104331422620935 MB/s).\n",
      "Processed 14400 documents (2535.8785337328063 docs/s, 2.1160166328972063 MB/s).\n",
      "Processed 14500 documents (2552.78504684195 docs/s, 2.1278256866285448 MB/s).\n",
      "Processed 14600 documents (2569.9787067698053 docs/s, 2.1411470427288575 MB/s).\n",
      "Processed 14700 documents (2587.0835958442194 docs/s, 2.1528644369658125 MB/s).\n",
      "Processed 14800 documents (2604.1907554220256 docs/s, 2.16607564892808 MB/s).\n",
      "Processed 14900 documents (2620.127628187576 docs/s, 2.178752664196176 MB/s).\n",
      "Processed 15000 documents (2637.2435722213877 docs/s, 2.1917196218455923 MB/s).\n"
     ]
    }
   ],
   "source": [
    "!python /opt/NeMo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \\\n",
    "--input=databricks-dolly-15k.jsonl \\\n",
    "--json-keys=context \\\n",
    "--tokenizer-library=megatron \\\n",
    "--vocab gpt2-vocab.json \\\n",
    "--dataset-impl mmap \\\n",
    "--tokenizer-type GPT2BPETokenizer \\\n",
    "--merge-file gpt2-merges.txt \\\n",
    "--output-prefix=hfbpe_gpt_training_data \\\n",
    "--append-eod \\\n",
    "--workers=32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac7c892",
   "metadata": {},
   "source": [
    "Train GPT-style model\n",
    "Once you have prepared the training data and tokenizer, you are ready to train the model.\n",
    "\n",
    "The following configuration has about 124M parameters and should fit on a single 16GB GPU using float16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ab80a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-21 12:16:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo I 2024-05-21 12:16:15 megatron_gpt_pretraining:34] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_gpt_pretraining:35] \n",
      "    name: megatron_gpt\n",
      "    restore_from_path: null\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: null\n",
      "      max_steps: 3000\n",
      "      log_every_n_steps: 5\n",
      "      val_check_interval: 300\n",
      "      limit_val_batches: 5\n",
      "      limit_test_batches: 5\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      benchmark: false\n",
      "      enable_model_summary: false\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: megatron_gpt\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      create_neptune_logger: false\n",
      "      neptune_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "        prefix: train\n",
      "        log_model_checkpoints: false\n",
      "        tags: null\n",
      "        description: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      resume_from_checkpoint: ${model.resume_from_checkpoint}\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_loss\n",
      "        save_top_k: 3\n",
      "        mode: min\n",
      "        always_save_nemo: false\n",
      "        save_nemo_on_train_end: false\n",
      "        filename: megatron_gpt--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "    model:\n",
      "      mcore_gpt: false\n",
      "      micro_batch_size: 6\n",
      "      global_batch_size: 192\n",
      "      rampup_batch_size: null\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      virtual_pipeline_model_parallel_size: null\n",
      "      encoder_seq_length: 1024\n",
      "      max_position_embeddings: 1024\n",
      "      num_layers: 12\n",
      "      hidden_size: 768\n",
      "      ffn_hidden_size: 3072\n",
      "      num_attention_heads: 12\n",
      "      init_method_std: 0.021\n",
      "      use_scaled_init_method: true\n",
      "      hidden_dropout: 0.1\n",
      "      attention_dropout: 0.1\n",
      "      ffn_dropout: 0.0\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: false\n",
      "      normalization: layernorm\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      do_layer_norm_weight_decay: false\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      persist_layer_norm: true\n",
      "      bias: true\n",
      "      activation: gelu\n",
      "      headscale: false\n",
      "      transformer_block_type: pre_ln\n",
      "      openai_gelu: false\n",
      "      normalize_attention_scores: true\n",
      "      position_embedding_type: learned_absolute\n",
      "      rotary_percentage: 1.0\n",
      "      attention_type: multihead\n",
      "      share_embeddings_and_output_weights: true\n",
      "      overlap_p2p_comm: false\n",
      "      batch_p2p_comm: true\n",
      "      seq_len_interpolation_factor: null\n",
      "      num_query_groups: null\n",
      "      tokenizer:\n",
      "        library: megatron\n",
      "        type: GPT2BPETokenizer\n",
      "        model: null\n",
      "        vocab_file: gpt2-vocab.json\n",
      "        merge_file: gpt2-merges.txt\n",
      "        delimiter: null\n",
      "        sentencepiece_legacy: false\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      hysteresis: 2\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      megatron_amp_O2: false\n",
      "      grad_allreduce_chunk_size_mb: 125\n",
      "      grad_div_ar_fusion: true\n",
      "      gradient_accumulation_fusion: false\n",
      "      bias_activation_fusion: true\n",
      "      bias_dropout_add_fusion: true\n",
      "      masked_softmax_fusion: true\n",
      "      get_attention_mask_from_fusion: true\n",
      "      apply_rope_fusion: false\n",
      "      seed: 1234\n",
      "      resume_from_checkpoint: null\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      apex_transformer_log_level: 30\n",
      "      gradient_as_bucket_view: true\n",
      "      sync_batch_comm: false\n",
      "      nccl_communicator_config_path: null\n",
      "      fsdp: false\n",
      "      fsdp_sharding_strategy: full\n",
      "      fsdp_grad_reduce_dtype: fp32\n",
      "      fsdp_sharded_checkpoint: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      num_micro_batches_with_partial_activation_checkpoints: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      sequence_parallel: false\n",
      "      transformer_engine: false\n",
      "      fp8: false\n",
      "      fp8_e4m3: false\n",
      "      fp8_hybrid: true\n",
      "      fp8_margin: 0\n",
      "      fp8_interval: 1\n",
      "      fp8_amax_history_len: 1024\n",
      "      fp8_amax_compute_algo: max\n",
      "      reduce_amax: true\n",
      "      use_emha: false\n",
      "      ub_tp_comm_overlap: false\n",
      "      ub_tp_comm_overlap_cfg: null\n",
      "      use_flash_attention: false\n",
      "      cpu_offloading: false\n",
      "      cpu_offloading_num_layers: ${sum:${.num_layers},-1}\n",
      "      cpu_offloading_activations: true\n",
      "      cpu_offloading_weights: true\n",
      "      sharp: false\n",
      "      enable_megatron_timers: false\n",
      "      megatron_timer_kwargs:\n",
      "        log_every_n_steps: 10\n",
      "        log_mode: minmax\n",
      "        barrier: false\n",
      "      data:\n",
      "        data_prefix:\n",
      "        - 1.0\n",
      "        - hfbpe_gpt_training_data_context_document\n",
      "        index_mapping_dir: null\n",
      "        data_impl: mmap\n",
      "        splits_string: 980,10,10\n",
      "        seq_length: 1024\n",
      "        skip_warmup: true\n",
      "        num_workers: 4\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        validation_drop_last: true\n",
      "        no_seqlen_plus_one_input_tokens: false\n",
      "        pad_samples_to_global_batch_size: false\n",
      "        shuffle_documents: true\n",
      "        exchange_indices_distributed: false\n",
      "        mock_dataset: false\n",
      "      nsys_profile:\n",
      "        enabled: false\n",
      "        start_step: 10\n",
      "        end_step: 10\n",
      "        ranks:\n",
      "        - 0\n",
      "        gen_shape: false\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0006\n",
      "        weight_decay: 0.1\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.95\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 150\n",
      "          constant_steps: 8000\n",
      "          min_lr: 6.0e-05\n",
      "      gc_interval: 0\n",
      "    \n",
      "[NeMo W 2024-05-21 12:16:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "      rank_zero_warn(\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2024-05-21 12:16:15 exp_manager:773] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
      "[NeMo W 2024-05-21 12:16:15 exp_manager:630] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/workspace/notebooks/nemo_experiments/megatron_gpt/checkpoints. Training from scratch.\n",
      "[NeMo I 2024-05-21 12:16:15 exp_manager:396] Experiments will be logged at /workspace/notebooks/nemo_experiments/megatron_gpt\n",
      "[NeMo I 2024-05-21 12:16:15 exp_manager:856] TensorboardLogger has been set up\n",
      "[NeMo W 2024-05-21 12:16:15 exp_manager:966] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 3000. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_init:253] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_init:259] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_init:264] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_init:267] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_init:284] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_init:287] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_init:288] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_init:299] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_init:300] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_init:310] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_init:314] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_init:315] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_init:344] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_init:356] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_init:362] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_init:363] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_init:364] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_init:365] Rank 0 has embedding rank: 0\n",
      "24-05-21 12:16:15 - PID:256 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 32\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 modelPT:258] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo I 2024-05-21 12:16:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /workspace/notebooks/gpt2-vocab.json, and merges file: /workspace/notebooks/gpt2-merges.txt\n",
      "[NeMo I 2024-05-21 12:16:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /workspace/notebooks/gpt2-vocab.json, merges_files: /workspace/notebooks/gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-21 12:16:15 megatron_base_model:574] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: add_qkv_bias in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: rotary_interleaved in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:15 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 12:16:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:153: UserWarning: The `batch_idx` argument in `MegatronGPTModel.on_train_batch_start` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2024-05-21 12:16:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:153: UserWarning: The `batch_idx` argument in `MegatronGPTModel.on_train_batch_end` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo I 2024-05-21 12:16:16 megatron_gpt_model:1342] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 1.24e+08. Total number of model parameters: 1.24e+08.\n",
      "[NeMo I 2024-05-21 12:16:16 megatron_gpt_model:1217] Building GPT datasets.\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] mock = False\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] Let split_matrix = [(0, 0.98), (0.98, 0.99), (0.99, 1.0)]\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] Load the _IndexReader from hfbpe_gpt_training_data_context_document.idx\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] \tExtract the sequence lengths\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] \tExtract the sequence pointers\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] \tExtract the document indices\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] > total number of sequences: 4467\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] > total number of documents: 4467\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] Load the GPTDataset train indices\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] \tLoad the document index from 2e8421114ec5bbb6c9562b97e2243ae3-GPTDataset-document_index.npy\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] \tLoad the sample index from 2e8421114ec5bbb6c9562b97e2243ae3-GPTDataset-sample_index.npy\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] \tLoad the shuffle index from 2e8421114ec5bbb6c9562b97e2243ae3-GPTDataset-shuffle_index.npy\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] > total number of samples: 579615\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] > total number of epochs: 521\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] Load the GPTDataset valid indices\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] \tLoad the document index from 09590e84250cee8c99ea05689515c0c2-GPTDataset-document_index.npy\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] \tLoad the sample index from 09590e84250cee8c99ea05689515c0c2-GPTDataset-sample_index.npy\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] \tLoad the shuffle index from 09590e84250cee8c99ea05689515c0c2-GPTDataset-shuffle_index.npy\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] > total number of samples: 10623\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] > total number of epochs: 989\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] Load the GPTDataset test indices\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] \tLoad the document index from 18cb7a36d50a4150e8cd087a85707a8a-GPTDataset-document_index.npy\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] \tLoad the sample index from 18cb7a36d50a4150e8cd087a85707a8a-GPTDataset-sample_index.npy\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] \tLoad the shuffle index from 18cb7a36d50a4150e8cd087a85707a8a-GPTDataset-shuffle_index.npy\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] > total number of samples: 967\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] > total number of epochs: 100\n",
      "[NeMo W 2024-05-21 12:16:16 utils:47] Building a BlendedDataset for a single MegatronDataset\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] \tBuild and save the dataset and dataset sample indexes\n",
      "[NeMo W 2024-05-21 12:16:16 utils:47] Unable to save the indexes because path_to_cache is None\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] > BlendedDataset length: 578880\n",
      "[NeMo W 2024-05-21 12:16:16 utils:47] Building a BlendedDataset for a single MegatronDataset\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] \tBuild and save the dataset and dataset sample indexes\n",
      "[NeMo W 2024-05-21 12:16:16 utils:47] Unable to save the indexes because path_to_cache is None\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] > BlendedDataset length: 10613\n",
      "[NeMo W 2024-05-21 12:16:16 utils:47] Building a BlendedDataset for a single MegatronDataset\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] \tBuild and save the dataset and dataset sample indexes\n",
      "[NeMo W 2024-05-21 12:16:16 utils:47] Unable to save the indexes because path_to_cache is None\n",
      "[NeMo I 2024-05-21 12:16:16 utils:47] > BlendedDataset length: 965\n",
      "[NeMo I 2024-05-21 12:16:16 megatron_gpt_model:1280] Length of train dataset: 578880\n",
      "[NeMo I 2024-05-21 12:16:16 megatron_gpt_model:1282] Length of val dataset: 10613\n",
      "[NeMo I 2024-05-21 12:16:16 megatron_gpt_model:1284] Length of test dataset: 965\n",
      "[NeMo I 2024-05-21 12:16:16 megatron_gpt_model:1285] Finished building GPT datasets.\n",
      "[NeMo I 2024-05-21 12:16:16 megatron_gpt_model:1384] Setting up train dataloader with len(len(self._train_ds)): 578880 and consumed samples: 0\n",
      "[NeMo I 2024-05-21 12:16:16 megatron_gpt_model:1294] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-05-21 12:16:16 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 578880 and consumed_samples: 0\n",
      "[NeMo I 2024-05-21 12:16:16 megatron_gpt_model:1392] Setting up validation dataloader with len(len(self._validation_ds)): 10613 and consumed samples: 0\n",
      "[NeMo I 2024-05-21 12:16:16 megatron_gpt_model:1294] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-05-21 12:16:16 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 10613 and consumed_samples: 0\n",
      "[NeMo I 2024-05-21 12:16:16 megatron_gpt_model:1412] Setting up test dataloader with len(len(self._test_ds)): 965 and consumed samples: 0\n",
      "[NeMo I 2024-05-21 12:16:16 megatron_gpt_model:1294] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-05-21 12:16:16 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 965 and consumed_samples: 0\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2024-05-21 12:16:16 nlp_overrides:228] Configuring DDP for model parallelism.\n",
      "[NeMo I 2024-05-21 12:16:16 modelPT:724] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.95]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0006\n",
      "        weight_decay: 0.1\n",
      "    \n",
      "    Parameter Group 1\n",
      "        betas: [0.9, 0.95]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0006\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2024-05-21 12:16:16 lr_scheduler:915] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f04d85706d0>\" \n",
      "    will be used during training (effective maximum steps = 3000) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 150\n",
      "    constant_steps: 8000\n",
      "    min_lr: 6.0e-05\n",
      "    max_steps: 3000\n",
      "    )\n",
      "Sanity Checking: 0it [00:00, ?it/s][NeMo W 2024-05-21 12:17:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Sanity Checking DataLoader 0: : 65it [00:06, 10.21it/s]                         [NeMo W 2024-05-21 12:17:21 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2024-05-21 12:18:20 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Epoch 0: :   0%|                                              | 0/3000 [00:00<?][NeMo W 2024-05-21 12:18:27 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2024-05-21 12:18:27 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('consumed_samples', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2024-05-21 12:18:28 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/megatron/clip_grads.py:86: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "      dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
      "    \n",
      "Epoch 0: :   4%| | 129/3000 [16:03<5:57:33, v_num=4, reduced_train_loss=5.960, g^C\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-3:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-3:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-3:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Process ForkProcess-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-1:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-4:\n",
      "Traceback (most recent call last):\n",
      "Process ForkProcess-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "[NeMo W 2024-05-21 12:51:12 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "      rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/utils/_process_posix.py:153\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# res is the index of the pattern that caused the match, so we\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# know whether we've finished (if we matched EOF) or not\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     res_idx \u001b[38;5;241m=\u001b[39m \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(child\u001b[38;5;241m.\u001b[39mbefore[out_size:]\u001b[38;5;241m.\u001b[39mdecode(enc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pexpect/spawnbase.py:383\u001b[0m, in \u001b[0;36mSpawnBase.expect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pexpect/expect.py:169\u001b[0m, in \u001b[0;36mExpecter.expect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Still have time left, so read more data\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m incoming \u001b[38;5;241m=\u001b[39m \u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_nonblocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspawn\u001b[38;5;241m.\u001b[39mdelayafterread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pexpect/pty_spawn.py:500\u001b[0m, in \u001b[0;36mspawn.read_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Because of the select(0) check above, we know that no data\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# is available right now. But if a non-zero timeout is given\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# (possibly timeout=None), we call select() with a timeout.\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (timeout \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(spawn, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mread_nonblocking(size)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pexpect/pty_spawn.py:450\u001b[0m, in \u001b[0;36mspawn.read_nonblocking.<locals>.select\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(timeout):\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect_ignore_interrupts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchild_fd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pexpect/utils.py:143\u001b[0m, in \u001b[0;36mselect_ignore_interrupts\u001b[0;34m(iwtd, owtd, ewtd, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43miwtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mewtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpython /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py       --config-path=conf/      --config-name=megatron_gpt_config      trainer.devices=1      trainer.num_nodes=1      trainer.max_epochs=null      trainer.max_steps=3000      trainer.val_check_interval=300      trainer.log_every_n_steps=5      trainer.limit_val_batches=5      trainer.limit_test_batches=5      trainer.accumulate_grad_batches=1      trainer.precision=16      model.micro_batch_size=6      model.global_batch_size=192      model.tensor_model_parallel_size=1      model.pipeline_model_parallel_size=1      model.max_position_embeddings=1024      model.encoder_seq_length=1024      model.hidden_size=768      model.ffn_hidden_size=3072      model.num_layers=12      model.num_attention_heads=12      model.init_method_std=0.021      model.hidden_dropout=0.1      model.layernorm_epsilon=1e-5      model.tokenizer.vocab_file=gpt2-vocab.json      model.tokenizer.merge_file=gpt2-merges.txt      model.data.data_prefix=[1.0,hfbpe_gpt_training_data_context_document]      model.data.num_workers=4      model.data.seq_length=1024      model.data.splits_string=\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m980,10,10\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m      model.optim.name=fused_adam      model.optim.lr=6e-4      model.optim.betas=[0.9,0.95]      model.optim.weight_decay=0.1      model.optim.sched.name=CosineAnnealing      model.optim.sched.warmup_steps=150      model.optim.sched.constant_steps=8000      model.optim.sched.min_lr=6e-5      exp_manager.resume_if_exists=True      exp_manager.resume_ignore_no_checkpoint=True      exp_manager.create_checkpoint_callback=True      exp_manager.checkpoint_callback_params.monitor=val_loss      exp_manager.checkpoint_callback_params.save_top_k=3      exp_manager.checkpoint_callback_params.mode=min      exp_manager.checkpoint_callback_params.always_save_nemo=False\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py:657\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m system(cmd)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/utils/_process_posix.py:177\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;66;03m# Ensure the subprocess really is terminated\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m         \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# add isalive check, to ensure exitstatus is set:\u001b[39;00m\n\u001b[1;32m    179\u001b[0m child\u001b[38;5;241m.\u001b[39misalive()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pexpect/pty_spawn.py:642\u001b[0m, in \u001b[0;36mspawn.terminate\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkill(signal\u001b[38;5;241m.\u001b[39mSIGHUP)\n\u001b[0;32m--> 642\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelayafterterminate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misalive():\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!python /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py  \\\n",
    "    --config-path=conf/ \\\n",
    "    --config-name=megatron_gpt_config \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.max_epochs=null \\\n",
    "    trainer.max_steps=3000 \\\n",
    "    trainer.val_check_interval=300 \\\n",
    "    trainer.log_every_n_steps=5 \\\n",
    "    trainer.limit_val_batches=5 \\\n",
    "    trainer.limit_test_batches=5 \\\n",
    "    trainer.accumulate_grad_batches=1 \\\n",
    "    trainer.precision=16 \\\n",
    "    model.micro_batch_size=6 \\\n",
    "    model.global_batch_size=192 \\\n",
    "    model.tensor_model_parallel_size=1 \\\n",
    "    model.pipeline_model_parallel_size=1 \\\n",
    "    model.max_position_embeddings=1024 \\\n",
    "    model.encoder_seq_length=1024 \\\n",
    "    model.hidden_size=768 \\\n",
    "    model.ffn_hidden_size=3072 \\\n",
    "    model.num_layers=12 \\\n",
    "    model.num_attention_heads=12 \\\n",
    "    model.init_method_std=0.021 \\\n",
    "    model.hidden_dropout=0.1 \\\n",
    "    model.layernorm_epsilon=1e-5 \\\n",
    "    model.tokenizer.vocab_file=gpt2-vocab.json \\\n",
    "    model.tokenizer.merge_file=gpt2-merges.txt \\\n",
    "    model.data.data_prefix=[1.0,hfbpe_gpt_training_data_context_document] \\\n",
    "    model.data.num_workers=4 \\\n",
    "    model.data.seq_length=1024 \\\n",
    "    model.data.splits_string=\\'980,10,10\\' \\\n",
    "    model.optim.name=fused_adam \\\n",
    "    model.optim.lr=6e-4 \\\n",
    "    model.optim.betas=[0.9,0.95] \\\n",
    "    model.optim.weight_decay=0.1 \\\n",
    "    model.optim.sched.name=CosineAnnealing \\\n",
    "    model.optim.sched.warmup_steps=150 \\\n",
    "    model.optim.sched.constant_steps=8000 \\\n",
    "    model.optim.sched.min_lr=6e-5 \\\n",
    "    exp_manager.resume_if_exists=True \\\n",
    "    exp_manager.resume_ignore_no_checkpoint=True \\\n",
    "    exp_manager.create_checkpoint_callback=True \\\n",
    "    exp_manager.checkpoint_callback_params.monitor=val_loss \\\n",
    "    exp_manager.checkpoint_callback_params.save_top_k=3 \\\n",
    "    exp_manager.checkpoint_callback_params.mode=min \\\n",
    "    exp_manager.checkpoint_callback_params.always_save_nemo=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3072fb1",
   "metadata": {},
   "source": [
    "Run this command in a separate terminal to monitor the training.\n",
    "tensorboard --logdir notebooks/nemo_experiments --bind_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
