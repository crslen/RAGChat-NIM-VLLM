{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3134e50",
   "metadata": {},
   "source": [
    "# 01. NeMo Framework Single Node Pre-training\n",
    "\n",
    "This notebook demonstrates how to train a GPT-style model with the NVIDIA NeMo Framework.\n",
    "\n",
    "The total training will take a considerable amount of time, but can be achieved on a GPU with as low as 16GB of GPU RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "812c3820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A40-16Q\n",
      "__CUDNN VERSION: 8907\n",
      "__Number CUDA Devices: 1\n",
      "__CUDA Device Name: NVIDIA A40-16Q\n",
      "__CUDA Device Total Memory [GB]: 16.966156288\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if not use_cuda:\n",
    "    exit()\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)\n",
    "print('Memory Usage:')\n",
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0371b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 20 20:58:28 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40-16Q                 On  |   00000000:03:00.0 Off |                    0 |\n",
      "| N/A   N/A    P8             N/A /  N/A  |       0MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33691899",
   "metadata": {},
   "source": [
    "## Prepare dataset and dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13d81cc",
   "metadata": {},
   "source": [
    "Get tokenizer files\n",
    "This step uses This playbook demonstrates how to train a GPT-style model with the NVIDIA NeMo Framework.\n",
    "\n",
    "The total training will take a considerable amount of time, but can be achieved on a GPU with as low as 16GB of GPU RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd46ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-20 20:58:35--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.122.144, 54.231.135.240, 52.217.48.126, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.122.144|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1042301 (1018K) [application/json]\n",
      "Saving to: ‘gpt2-vocab.json.1’\n",
      "\n",
      "gpt2-vocab.json.1   100%[===================>]   1018K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-05-20 20:58:36 (7.92 MB/s) - ‘gpt2-vocab.json.1’ saved [1042301/1042301]\n",
      "\n",
      "--2024-05-20 20:58:36--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.36.152, 52.217.122.144, 54.231.135.240, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.36.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 456318 (446K) [text/plain]\n",
      "Saving to: ‘gpt2-merges.txt.1’\n",
      "\n",
      "gpt2-merges.txt.1   100%[===================>] 445.62K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-05-20 20:58:36 (4.57 MB/s) - ‘gpt2-merges.txt.1’ saved [456318/456318]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32786230",
   "metadata": {},
   "source": [
    "In this example, we use dataset used in [official LoRA example](https://github.com/microsoft/LoRA).\n",
    "\n",
    "Download dataset from official repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "201987e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-20 20:29:22--  https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\n",
      "Resolving huggingface.co (huggingface.co)... 18.172.134.88, 18.172.134.4, 18.172.134.124, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.172.134.88|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1716496162&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjQ5NjE2Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=feMInyzi0BaqKDqyFj8-co8P%7Ez2nd-vsVyGIotud9Xyw3FIPgFSTmTrEXDZMV%7ERiK5cutL2XWilhRg413JnQI45kcvFpNejp-mk8d3MONSWSIfjNcOqfWASJP3qBkLvcnH6pP0Zpe1vKpuu74FI1wQe0oHsdHPPXWSRM0B7KSTUdoH3CtUedcZIaDds05u0Lpq2vJR-4gE0-v9DuLKt1Yx%7EQecOpdQVeImrEWzOK-G5lvIDNhHCC-EWcvdeC1WvnlBIdPnoRMk9MAiQFPk5bV%7EN-abO8zhQ6E451Wn%7EYKCJgk9F5L3vJq-eSxZekk-Stqrg2UVvupI6jgRsrMKtM2A__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2024-05-20 20:29:22--  https://cdn-lfs.huggingface.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1716496162&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjQ5NjE2Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=feMInyzi0BaqKDqyFj8-co8P%7Ez2nd-vsVyGIotud9Xyw3FIPgFSTmTrEXDZMV%7ERiK5cutL2XWilhRg413JnQI45kcvFpNejp-mk8d3MONSWSIfjNcOqfWASJP3qBkLvcnH6pP0Zpe1vKpuu74FI1wQe0oHsdHPPXWSRM0B7KSTUdoH3CtUedcZIaDds05u0Lpq2vJR-4gE0-v9DuLKt1Yx%7EQecOpdQVeImrEWzOK-G5lvIDNhHCC-EWcvdeC1WvnlBIdPnoRMk9MAiQFPk5bV%7EN-abO8zhQ6E451Wn%7EYKCJgk9F5L3vJq-eSxZekk-Stqrg2UVvupI6jgRsrMKtM2A__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.154.185.26, 18.154.185.27, 18.154.185.94, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.154.185.26|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13085339 (12M) [text/plain]\n",
      "Saving to: ‘databricks-dolly-15k.jsonl.1’\n",
      "\n",
      "databricks-dolly-15 100%[===================>]  12.48M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-05-20 20:29:22 (107 MB/s) - ‘databricks-dolly-15k.jsonl.1’ saved [13085339/13085339]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d0f9d2",
   "metadata": {},
   "source": [
    "Step 4: Convert training data into memory map format\n",
    "\n",
    "The memory map format makes training more efficient, especially with many nodes and GPUs. This step will also tokenize data using the tokenizer model from Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d95d7feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-20 20:59:14 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:14 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "Vocab size: 50257\n",
      "Output prefix: hfbpe_gpt_training_data\n",
      "Time to startup: 0.2828195095062256\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:15 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:16 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:16 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:16 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:16 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:16 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:16 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:16 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 20:59:16 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: gpt2-vocab.json, and merges file: gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 20:59:16 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: gpt2-vocab.json, merges_files: gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "Processing file databricks-dolly-15k.jsonl 1/1\n",
      "Processed 100 documents (66.00684539261076 docs/s, 0.06012765749179263 MB/s).\n",
      "Processed 200 documents (131.9292100422495 docs/s, 0.10631201229991873 MB/s).\n",
      "Processed 300 documents (197.78432225662127 docs/s, 0.15659321313512115 MB/s).\n",
      "Processed 400 documents (244.36898207071096 docs/s, 0.19235933270080108 MB/s).\n",
      "Processed 500 documents (295.058556507075 docs/s, 0.23585021003635698 MB/s).\n",
      "Processed 600 documents (335.7812472480582 docs/s, 0.2663036371797273 MB/s).\n",
      "Processed 700 documents (391.4938987355309 docs/s, 0.31498267020343057 MB/s).\n",
      "Processed 800 documents (374.4022573195179 docs/s, 0.3028457284838633 MB/s).\n",
      "Processed 900 documents (420.97359565197223 docs/s, 0.3420623690756159 MB/s).\n",
      "Processed 1000 documents (449.71694761037907 docs/s, 0.3743561118365199 MB/s).\n",
      "Processed 1100 documents (494.46215517745827 docs/s, 0.41163611234775443 MB/s).\n",
      "Processed 1200 documents (539.1211894682755 docs/s, 0.44979637139295764 MB/s).\n",
      "Processed 1300 documents (583.791323593762 docs/s, 0.48472816784374 MB/s).\n",
      "Processed 1400 documents (628.513024088149 docs/s, 0.5149614240039337 MB/s).\n",
      "Processed 1500 documents (673.1825068693664 docs/s, 0.5501776191332186 MB/s).\n",
      "Processed 1600 documents (691.1098148935399 docs/s, 0.57086080930113 MB/s).\n",
      "Processed 1700 documents (733.978849172298 docs/s, 0.6043657139845479 MB/s).\n",
      "Processed 1800 documents (760.281714451385 docs/s, 0.6263577651077818 MB/s).\n",
      "Processed 1900 documents (749.5988808848174 docs/s, 0.640471078864059 MB/s).\n",
      "Processed 2000 documents (788.6958034232633 docs/s, 0.6696008386572083 MB/s).\n",
      "Processed 2100 documents (827.734938776262 docs/s, 0.7057537288788958 MB/s).\n",
      "Processed 2200 documents (850.2907170147364 docs/s, 0.7257285986332274 MB/s).\n",
      "Processed 2300 documents (883.7178573699102 docs/s, 0.7551087523884082 MB/s).\n",
      "Processed 2400 documents (921.7250325857497 docs/s, 0.7868256054631434 MB/s).\n",
      "Processed 2500 documents (938.5777813342986 docs/s, 0.8013303297440818 MB/s).\n",
      "Processed 2600 documents (975.6926066338244 docs/s, 0.8291651572251304 MB/s).\n",
      "Processed 2700 documents (1012.777865917493 docs/s, 0.8563899971936394 MB/s).\n",
      "Processed 2800 documents (984.9932768463176 docs/s, 0.8365012225135738 MB/s).\n",
      "Processed 2900 documents (1019.742687599278 docs/s, 0.8637665806421215 MB/s).\n",
      "Processed 3000 documents (1024.6671009771987 docs/s, 0.8712439739413681 MB/s).\n",
      "Processed 3100 documents (1058.547000544972 docs/s, 0.8971305355494554 MB/s).\n",
      "Processed 3200 documents (1092.366498012473 docs/s, 0.9245545470993158 MB/s).\n",
      "Processed 3300 documents (1126.1845328908832 docs/s, 0.9472100512019551 MB/s).\n",
      "Processed 3400 documents (1159.9833607536011 docs/s, 0.9728213169765051 MB/s).\n",
      "Processed 3500 documents (1193.7849290351626 docs/s, 1.000941687275902 MB/s).\n",
      "Processed 3600 documents (1196.9486774820018 docs/s, 1.0027154205207789 MB/s).\n",
      "Processed 3700 documents (1229.7554558528807 docs/s, 1.0272221901290866 MB/s).\n",
      "Processed 3800 documents (1262.5159771487959 docs/s, 1.0504415301308272 MB/s).\n",
      "Processed 3900 documents (1295.1003426938378 docs/s, 1.073414577836725 MB/s).\n",
      "Processed 4000 documents (1290.799037941248 docs/s, 1.0701644149205738 MB/s).\n",
      "Processed 4100 documents (1322.6165935073473 docs/s, 1.0933265671890662 MB/s).\n",
      "Processed 4200 documents (1223.391340460492 docs/s, 1.0166309938344567 MB/s).\n",
      "Processed 4300 documents (1252.0715491386254 docs/s, 1.0398884129122121 MB/s).\n",
      "Processed 4400 documents (1280.762350081593 docs/s, 1.061039722990744 MB/s).\n",
      "Processed 4500 documents (1309.4204910945673 docs/s, 1.0859028723646553 MB/s).\n",
      "Processed 4600 documents (1257.094009253101 docs/s, 1.0470848979302494 MB/s).\n",
      "Processed 4700 documents (1284.012957353066 docs/s, 1.069371784102042 MB/s).\n",
      "Processed 4800 documents (1310.9168082135247 docs/s, 1.0945546903070213 MB/s).\n",
      "Processed 4900 documents (1337.7921175668876 docs/s, 1.1188850808904403 MB/s).\n",
      "Processed 5000 documents (1364.7741846390668 docs/s, 1.1378145659463867 MB/s).\n",
      "Processed 5100 documents (1391.6681879567693 docs/s, 1.1583158791588635 MB/s).\n",
      "Processed 5200 documents (1418.5594153009613 docs/s, 1.1788097069916197 MB/s).\n",
      "Processed 5300 documents (1445.380831174509 docs/s, 1.2013691377297082 MB/s).\n",
      "Processed 5400 documents (1472.2038986824602 docs/s, 1.2229194137351278 MB/s).\n",
      "Processed 5500 documents (1493.4071374857836 docs/s, 1.2418350531054299 MB/s).\n",
      "Processed 5600 documents (1518.8587398985146 docs/s, 1.263949312966437 MB/s).\n",
      "Processed 5700 documents (1534.659164911619 docs/s, 1.2753952444022008 MB/s).\n",
      "Processed 5800 documents (1561.0228677651146 docs/s, 1.3006985567045768 MB/s).\n",
      "Processed 5900 documents (1586.7277649089829 docs/s, 1.3175594096538146 MB/s).\n",
      "Processed 6000 documents (1611.6980663714792 docs/s, 1.3362631783143823 MB/s).\n",
      "Processed 6100 documents (1635.152312747683 docs/s, 1.3538980211945844 MB/s).\n",
      "Processed 6200 documents (1660.1840582604304 docs/s, 1.3699623129501162 MB/s).\n",
      "Processed 6300 documents (1686.5760708297587 docs/s, 1.3892431969668315 MB/s).\n",
      "Processed 6400 documents (1704.3346794994668 docs/s, 1.4057039797878892 MB/s).\n",
      "Processed 6500 documents (1727.4654834502983 docs/s, 1.426646585646861 MB/s).\n",
      "Processed 6600 documents (1748.416765606579 docs/s, 1.441957419899725 MB/s).\n",
      "Processed 6700 documents (1773.0170872974657 docs/s, 1.4601573894616608 MB/s).\n",
      "Processed 6800 documents (1795.6595699082736 docs/s, 1.4789059983130868 MB/s).\n",
      "Processed 6900 documents (1747.2474633222716 docs/s, 1.4404397982708341 MB/s).\n",
      "Processed 7000 documents (1772.162146093489 docs/s, 1.4586040481294444 MB/s).\n",
      "Processed 7100 documents (1797.1113511050046 docs/s, 1.47543581501717 MB/s).\n",
      "Processed 7200 documents (1821.7245758210568 docs/s, 1.5027180497869868 MB/s).\n",
      "Processed 7300 documents (1844.109263393294 docs/s, 1.516474280966539 MB/s).\n",
      "Processed 7400 documents (1868.9827922441957 docs/s, 1.5349455952237436 MB/s).\n",
      "Processed 7500 documents (1893.7844245097604 docs/s, 1.5562741909587394 MB/s).\n",
      "Processed 7600 documents (1918.562369401815 docs/s, 1.5741909638424416 MB/s).\n",
      "Processed 7700 documents (1939.7018652311879 docs/s, 1.5909298552425115 MB/s).\n",
      "Processed 7800 documents (1964.422754409471 docs/s, 1.6104973460490484 MB/s).\n",
      "Processed 7900 documents (1989.022445488422 docs/s, 1.6324039383542936 MB/s).\n",
      "Processed 8000 documents (2013.793171328117 docs/s, 1.6527844920357049 MB/s).\n",
      "Processed 8100 documents (2023.923089419648 docs/s, 1.6594599486016532 MB/s).\n",
      "Processed 8200 documents (2028.2386348424288 docs/s, 1.6619797747531995 MB/s).\n",
      "Processed 8300 documents (2052.464167521744 docs/s, 1.6818126398319764 MB/s).\n",
      "Processed 8400 documents (1988.996637655135 docs/s, 1.6305455981644765 MB/s).\n",
      "Processed 8500 documents (2012.023975713664 docs/s, 1.6466394992478244 MB/s).\n",
      "Processed 8600 documents (2035.213817263138 docs/s, 1.6653374297512697 MB/s).\n",
      "Processed 8700 documents (2058.38399365062 docs/s, 1.6843650746867265 MB/s).\n",
      "Processed 8800 documents (2081.5812987983295 docs/s, 1.7039423061088723 MB/s).\n",
      "Processed 8900 documents (2089.648752227535 docs/s, 1.7131781982380794 MB/s).\n",
      "Processed 9000 documents (2111.406456891551 docs/s, 1.7271319333149089 MB/s).\n",
      "Processed 9100 documents (2134.214988567674 docs/s, 1.7488520564571437 MB/s).\n",
      "Processed 9200 documents (2156.9942897300443 docs/s, 1.7691323446204992 MB/s).\n",
      "Processed 9300 documents (2179.8819175101335 docs/s, 1.7874632539516795 MB/s).\n",
      "Processed 9400 documents (2198.990281073275 docs/s, 1.8033974022253796 MB/s).\n",
      "Processed 9500 documents (2221.7900978452435 docs/s, 1.8213446233850004 MB/s).\n",
      "Processed 9600 documents (2233.075093953126 docs/s, 1.833715224144284 MB/s).\n",
      "Processed 9700 documents (2215.2343836990412 docs/s, 1.821532733863305 MB/s).\n",
      "Processed 9800 documents (2225.915896985122 docs/s, 1.8300512173953398 MB/s).\n",
      "Processed 9900 documents (2236.289770601496 docs/s, 1.8378489408787047 MB/s).\n",
      "Processed 10000 documents (2258.359213650442 docs/s, 1.8554960543229018 MB/s).\n",
      "Processed 10100 documents (2280.367936042316 docs/s, 1.8763771372963594 MB/s).\n",
      "Processed 10200 documents (2302.5316119830736 docs/s, 1.8895491614135882 MB/s).\n",
      "Processed 10300 documents (2263.26902477014 docs/s, 1.8616658204990637 MB/s).\n",
      "Processed 10400 documents (2284.5174690523204 docs/s, 1.8829196335764995 MB/s).\n",
      "Processed 10500 documents (2305.9654028156847 docs/s, 1.8993532172239784 MB/s).\n",
      "Processed 10600 documents (2322.501833177863 docs/s, 1.912721980117314 MB/s).\n",
      "Processed 10700 documents (2310.7989704576034 docs/s, 1.9060057532282357 MB/s).\n",
      "Processed 10800 documents (2314.133160293724 docs/s, 1.9082231056410597 MB/s).\n",
      "Processed 10900 documents (2335.0372392103536 docs/s, 1.92792986233173 MB/s).\n",
      "Processed 11000 documents (2356.014366489276 docs/s, 1.9472755522015908 MB/s).\n",
      "Processed 11100 documents (2376.617608097395 docs/s, 1.9627762001193496 MB/s).\n",
      "Processed 11200 documents (2397.5574989464544 docs/s, 1.979295739949439 MB/s).\n",
      "Processed 11300 documents (2398.456264018851 docs/s, 1.9826266951916949 MB/s).\n",
      "Processed 11400 documents (2375.5490533935654 docs/s, 1.9648082378353822 MB/s).\n",
      "Processed 11500 documents (2310.9039875595554 docs/s, 1.9144015163253048 MB/s).\n",
      "Processed 11600 documents (2330.4559546319592 docs/s, 1.930941684962343 MB/s).\n",
      "Processed 11700 documents (2350.111808247825 docs/s, 1.9466050127372423 MB/s).\n",
      "Processed 11800 documents (2369.7370261930014 docs/s, 1.9623996724977077 MB/s).\n",
      "Processed 11900 documents (2389.418320016265 docs/s, 1.9765265279952848 MB/s).\n",
      "Processed 12000 documents (2409.1862956108343 docs/s, 1.9897165146395277 MB/s).\n",
      "Processed 12100 documents (2385.4682711024498 docs/s, 1.974453235263137 MB/s).\n",
      "Processed 12200 documents (2404.454601392897 docs/s, 1.9900156685052706 MB/s).\n",
      "Processed 12300 documents (2423.4846509012987 docs/s, 2.0066871155960793 MB/s).\n",
      "Processed 12400 documents (2441.2879604695254 docs/s, 2.0229616218872732 MB/s).\n",
      "Processed 12500 documents (2460.3318140526594 docs/s, 2.041195616295081 MB/s).\n",
      "Processed 12600 documents (2479.4947085060494 docs/s, 2.0563997197537613 MB/s).\n",
      "Processed 12700 documents (2498.5238174373662 docs/s, 2.075219516940543 MB/s).\n",
      "Processed 12800 documents (2516.1739935086784 docs/s, 2.0934475822892997 MB/s).\n",
      "Processed 12900 documents (2533.8152902706897 docs/s, 2.1073441112871145 MB/s).\n",
      "Processed 13000 documents (2516.4559964795612 docs/s, 2.0925379671313435 MB/s).\n",
      "Processed 13100 documents (2535.154984118307 docs/s, 2.1101051972414826 MB/s).\n",
      "Processed 13200 documents (2553.9207209401447 docs/s, 2.1230327386389005 MB/s).\n",
      "Processed 13300 documents (2572.6270611848513 docs/s, 2.137023614809346 MB/s).\n",
      "Processed 13400 documents (2591.3441483988845 docs/s, 2.1513537458889305 MB/s).\n",
      "Processed 13500 documents (2524.480095730599 docs/s, 2.0981525759921817 MB/s).\n",
      "Processed 13600 documents (2528.9130452998024 docs/s, 2.103927590115457 MB/s).\n",
      "Processed 13700 documents (2545.7103434221267 docs/s, 2.1188253184521137 MB/s).\n",
      "Processed 13800 documents (2563.6886178468203 docs/s, 2.1345857131663895 MB/s).\n",
      "Processed 13900 documents (2581.8595360760823 docs/s, 2.149234150706897 MB/s).\n",
      "Processed 14000 documents (2599.9888154919695 docs/s, 2.165254692633912 MB/s).\n",
      "Processed 14100 documents (2618.01742960765 docs/s, 2.1819439683750317 MB/s).\n",
      "Processed 14200 documents (2636.053272257664 docs/s, 2.201691427685228 MB/s).\n",
      "Processed 14300 documents (2654.136338028867 docs/s, 2.217517449776068 MB/s).\n",
      "Processed 14400 documents (2672.339247659988 docs/s, 2.229883735191692 MB/s).\n",
      "Processed 14500 documents (2690.488963902159 docs/s, 2.242606181849857 MB/s).\n",
      "Processed 14600 documents (2708.6624081322807 docs/s, 2.2566897109481525 MB/s).\n",
      "Processed 14700 documents (2726.7411313414386 docs/s, 2.2690816871579704 MB/s).\n",
      "Processed 14800 documents (2744.8144241147947 docs/s, 2.2830415446805876 MB/s).\n",
      "Processed 14900 documents (2762.1742860957797 docs/s, 2.2968707783781714 MB/s).\n",
      "Processed 15000 documents (2780.13742631579 docs/s, 2.310473637272405 MB/s).\n"
     ]
    }
   ],
   "source": [
    "!python /opt/NeMo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \\\n",
    "--input=databricks-dolly-15k.jsonl \\\n",
    "--json-keys=context \\\n",
    "--tokenizer-library=megatron \\\n",
    "--vocab gpt2-vocab.json \\\n",
    "--dataset-impl mmap \\\n",
    "--tokenizer-type GPT2BPETokenizer \\\n",
    "--merge-file gpt2-merges.txt \\\n",
    "--output-prefix=hfbpe_gpt_training_data \\\n",
    "--append-eod \\\n",
    "--workers=32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa573f0",
   "metadata": {},
   "source": [
    "Train GPT-style model\n",
    "Once you have prepared the training data and tokenizer, you are ready to train the model.\n",
    "\n",
    "The following configuration has about 124M parameters and should fit on a single 16GB GPU using float16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e71e83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-20 21:00:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo I 2024-05-20 21:00:09 megatron_gpt_pretraining:34] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-05-20 21:00:09 megatron_gpt_pretraining:35] \n",
      "    name: megatron_gpt\n",
      "    restore_from_path: null\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: null\n",
      "      max_steps: 300000\n",
      "      log_every_n_steps: 50\n",
      "      val_check_interval: 300\n",
      "      limit_val_batches: 50\n",
      "      limit_test_batches: 50\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      benchmark: false\n",
      "      enable_model_summary: false\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: megatron_gpt\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      create_neptune_logger: false\n",
      "      neptune_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "        prefix: train\n",
      "        log_model_checkpoints: false\n",
      "        tags: null\n",
      "        description: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      resume_from_checkpoint: ${model.resume_from_checkpoint}\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_loss\n",
      "        save_top_k: 3\n",
      "        mode: min\n",
      "        always_save_nemo: false\n",
      "        save_nemo_on_train_end: false\n",
      "        filename: megatron_gpt--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "    model:\n",
      "      mcore_gpt: false\n",
      "      micro_batch_size: 6\n",
      "      global_batch_size: 192\n",
      "      rampup_batch_size: null\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      virtual_pipeline_model_parallel_size: null\n",
      "      encoder_seq_length: 1024\n",
      "      max_position_embeddings: 1024\n",
      "      num_layers: 12\n",
      "      hidden_size: 768\n",
      "      ffn_hidden_size: 3072\n",
      "      num_attention_heads: 12\n",
      "      init_method_std: 0.021\n",
      "      use_scaled_init_method: true\n",
      "      hidden_dropout: 0.1\n",
      "      attention_dropout: 0.1\n",
      "      ffn_dropout: 0.0\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: false\n",
      "      normalization: layernorm\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      do_layer_norm_weight_decay: false\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      persist_layer_norm: true\n",
      "      bias: true\n",
      "      activation: gelu\n",
      "      headscale: false\n",
      "      transformer_block_type: pre_ln\n",
      "      openai_gelu: false\n",
      "      normalize_attention_scores: true\n",
      "      position_embedding_type: learned_absolute\n",
      "      rotary_percentage: 1.0\n",
      "      attention_type: multihead\n",
      "      share_embeddings_and_output_weights: true\n",
      "      overlap_p2p_comm: false\n",
      "      batch_p2p_comm: true\n",
      "      seq_len_interpolation_factor: null\n",
      "      num_query_groups: null\n",
      "      tokenizer:\n",
      "        library: megatron\n",
      "        type: GPT2BPETokenizer\n",
      "        model: null\n",
      "        vocab_file: gpt2-vocab.json\n",
      "        merge_file: gpt2-merges.txt\n",
      "        delimiter: null\n",
      "        sentencepiece_legacy: false\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      hysteresis: 2\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      megatron_amp_O2: false\n",
      "      grad_allreduce_chunk_size_mb: 125\n",
      "      grad_div_ar_fusion: true\n",
      "      gradient_accumulation_fusion: false\n",
      "      bias_activation_fusion: true\n",
      "      bias_dropout_add_fusion: true\n",
      "      masked_softmax_fusion: true\n",
      "      get_attention_mask_from_fusion: true\n",
      "      apply_rope_fusion: false\n",
      "      seed: 1234\n",
      "      resume_from_checkpoint: null\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      apex_transformer_log_level: 30\n",
      "      gradient_as_bucket_view: true\n",
      "      sync_batch_comm: false\n",
      "      nccl_communicator_config_path: null\n",
      "      fsdp: false\n",
      "      fsdp_sharding_strategy: full\n",
      "      fsdp_grad_reduce_dtype: fp32\n",
      "      fsdp_sharded_checkpoint: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      num_micro_batches_with_partial_activation_checkpoints: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      sequence_parallel: false\n",
      "      transformer_engine: false\n",
      "      fp8: false\n",
      "      fp8_e4m3: false\n",
      "      fp8_hybrid: true\n",
      "      fp8_margin: 0\n",
      "      fp8_interval: 1\n",
      "      fp8_amax_history_len: 1024\n",
      "      fp8_amax_compute_algo: max\n",
      "      reduce_amax: true\n",
      "      use_emha: false\n",
      "      ub_tp_comm_overlap: false\n",
      "      ub_tp_comm_overlap_cfg: null\n",
      "      use_flash_attention: false\n",
      "      cpu_offloading: false\n",
      "      cpu_offloading_num_layers: ${sum:${.num_layers},-1}\n",
      "      cpu_offloading_activations: true\n",
      "      cpu_offloading_weights: true\n",
      "      sharp: false\n",
      "      enable_megatron_timers: false\n",
      "      megatron_timer_kwargs:\n",
      "        log_every_n_steps: 10\n",
      "        log_mode: minmax\n",
      "        barrier: false\n",
      "      data:\n",
      "        data_prefix:\n",
      "        - 1.0\n",
      "        - hfbpe_gpt_training_data_context_document\n",
      "        index_mapping_dir: null\n",
      "        data_impl: mmap\n",
      "        splits_string: 980,10,10\n",
      "        seq_length: 1024\n",
      "        skip_warmup: true\n",
      "        num_workers: 2\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        validation_drop_last: true\n",
      "        no_seqlen_plus_one_input_tokens: false\n",
      "        pad_samples_to_global_batch_size: false\n",
      "        shuffle_documents: true\n",
      "        exchange_indices_distributed: false\n",
      "        mock_dataset: false\n",
      "      nsys_profile:\n",
      "        enabled: false\n",
      "        start_step: 10\n",
      "        end_step: 10\n",
      "        ranks:\n",
      "        - 0\n",
      "        gen_shape: false\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0006\n",
      "        weight_decay: 0.1\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.95\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 750\n",
      "          constant_steps: 80000\n",
      "          min_lr: 6.0e-05\n",
      "      gc_interval: 0\n",
      "    \n",
      "[NeMo W 2024-05-20 21:00:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "      rank_zero_warn(\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2024-05-20 21:00:10 exp_manager:773] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
      "[NeMo W 2024-05-20 21:00:10 exp_manager:630] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/workspace/notebooks/nemo_experiments/megatron_gpt/checkpoints. Training from scratch.\n",
      "[NeMo I 2024-05-20 21:00:10 exp_manager:396] Experiments will be logged at /workspace/notebooks/nemo_experiments/megatron_gpt\n",
      "[NeMo I 2024-05-20 21:00:10 exp_manager:856] TensorboardLogger has been set up\n",
      "[NeMo W 2024-05-20 21:00:10 exp_manager:966] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 300000. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_init:253] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_init:259] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_init:264] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_init:267] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_init:284] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_init:287] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_init:288] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_init:299] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_init:300] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_init:310] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_init:314] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_init:315] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_init:344] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_init:356] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_init:362] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_init:363] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_init:364] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_init:365] Rank 0 has embedding rank: 0\n",
      "24-05-20 21:00:10 - PID:244 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 32\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 modelPT:258] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo I 2024-05-20 21:00:10 tokenizer_utils:198] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /workspace/notebooks/gpt2-vocab.json, and merges file: /workspace/notebooks/gpt2-merges.txt\n",
      "[NeMo I 2024-05-20 21:00:10 tokenizer_utils:127] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /workspace/notebooks/gpt2-vocab.json, merges_files: /workspace/notebooks/gpt2-merges.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_base_model:574] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:1139] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: add_qkv_bias in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: rotary_interleaved in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 megatron_base_model:546] The model: MegatronGPTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-20 21:00:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:153: UserWarning: The `batch_idx` argument in `MegatronGPTModel.on_train_batch_start` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2024-05-20 21:00:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:153: UserWarning: The `batch_idx` argument in `MegatronGPTModel.on_train_batch_end` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_gpt_model:1342] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 1.24e+08. Total number of model parameters: 1.24e+08.\n",
      "[NeMo I 2024-05-20 21:00:10 megatron_gpt_model:1217] Building GPT datasets.\n",
      "[NeMo I 2024-05-20 21:00:10 utils:47] mock = False\n",
      "[NeMo I 2024-05-20 21:00:10 utils:47] Let split_matrix = [(0, 0.98), (0.98, 0.99), (0.99, 1.0)]\n",
      "[NeMo I 2024-05-20 21:00:10 utils:47] Load the _IndexReader from hfbpe_gpt_training_data_context_document.idx\n",
      "[NeMo I 2024-05-20 21:00:10 utils:47] \tExtract the sequence lengths\n",
      "[NeMo I 2024-05-20 21:00:10 utils:47] \tExtract the sequence pointers\n",
      "[NeMo I 2024-05-20 21:00:10 utils:47] \tExtract the document indices\n",
      "[NeMo I 2024-05-20 21:00:10 utils:47] > total number of sequences: 4467\n",
      "[NeMo I 2024-05-20 21:00:10 utils:47] > total number of documents: 4467\n",
      "[NeMo I 2024-05-20 21:00:10 utils:47] Load the GPTDataset train indices\n",
      "[NeMo I 2024-05-20 21:00:10 utils:47] \tLoad the document index from 7374d6704f88d8dca613de69c09a3d44-GPTDataset-document_index.npy\n",
      "[NeMo I 2024-05-20 21:00:10 utils:47] \tLoad the sample index from 7374d6704f88d8dca613de69c09a3d44-GPTDataset-sample_index.npy\n",
      "[NeMo I 2024-05-20 21:00:10 utils:47] \tLoad the shuffle index from 7374d6704f88d8dca613de69c09a3d44-GPTDataset-shuffle_index.npy\n",
      "[NeMo I 2024-05-20 21:00:10 utils:47] > total number of samples: 57888079\n",
      "[NeMo I 2024-05-20 21:00:10 utils:47] > total number of epochs: 52034\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] Load the GPTDataset valid indices\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] \tLoad the document index from 78d5df0692a7d9731fff00f56c7f7517-GPTDataset-document_index.npy\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] \tLoad the sample index from 78d5df0692a7d9731fff00f56c7f7517-GPTDataset-sample_index.npy\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] \tLoad the shuffle index from 78d5df0692a7d9731fff00f56c7f7517-GPTDataset-shuffle_index.npy\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] > total number of samples: 9657648\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] > total number of epochs: 899121\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] Load the GPTDataset test indices\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] \tLoad the document index from a98e5212e6d9f5e5b0a13d8a33be91d4-GPTDataset-document_index.npy\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] \tLoad the sample index from a98e5212e6d9f5e5b0a13d8a33be91d4-GPTDataset-sample_index.npy\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] \tLoad the shuffle index from a98e5212e6d9f5e5b0a13d8a33be91d4-GPTDataset-shuffle_index.npy\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] > total number of samples: 9656\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] > total number of epochs: 998\n",
      "[NeMo W 2024-05-20 21:00:11 utils:47] Building a BlendedDataset for a single MegatronDataset\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] \tBuild and save the dataset and dataset sample indexes\n",
      "[NeMo W 2024-05-20 21:00:11 utils:47] Unable to save the indexes because path_to_cache is None\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] > BlendedDataset length: 57888000\n",
      "[NeMo W 2024-05-20 21:00:11 utils:47] Building a BlendedDataset for a single MegatronDataset\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] \tBuild and save the dataset and dataset sample indexes\n",
      "[NeMo W 2024-05-20 21:00:11 utils:47] Unable to save the indexes because path_to_cache is None\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] > BlendedDataset length: 9657648\n",
      "[NeMo W 2024-05-20 21:00:11 utils:47] Building a BlendedDataset for a single MegatronDataset\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] \tBuild and save the dataset and dataset sample indexes\n",
      "[NeMo W 2024-05-20 21:00:11 utils:47] Unable to save the indexes because path_to_cache is None\n",
      "[NeMo I 2024-05-20 21:00:11 utils:47] > BlendedDataset length: 9648\n",
      "[NeMo I 2024-05-20 21:00:11 megatron_gpt_model:1280] Length of train dataset: 57888000\n",
      "[NeMo I 2024-05-20 21:00:11 megatron_gpt_model:1282] Length of val dataset: 9657648\n",
      "[NeMo I 2024-05-20 21:00:11 megatron_gpt_model:1284] Length of test dataset: 9648\n",
      "[NeMo I 2024-05-20 21:00:11 megatron_gpt_model:1285] Finished building GPT datasets.\n",
      "[NeMo I 2024-05-20 21:00:11 megatron_gpt_model:1384] Setting up train dataloader with len(len(self._train_ds)): 57888000 and consumed samples: 0\n",
      "[NeMo I 2024-05-20 21:00:11 megatron_gpt_model:1294] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-05-20 21:00:11 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 57888000 and consumed_samples: 0\n",
      "[NeMo I 2024-05-20 21:00:11 megatron_gpt_model:1392] Setting up validation dataloader with len(len(self._validation_ds)): 9657648 and consumed samples: 0\n",
      "[NeMo I 2024-05-20 21:00:11 megatron_gpt_model:1294] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-05-20 21:00:11 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 9657648 and consumed_samples: 0\n",
      "[NeMo I 2024-05-20 21:00:11 megatron_gpt_model:1412] Setting up test dataloader with len(len(self._test_ds)): 9648 and consumed samples: 0\n",
      "[NeMo I 2024-05-20 21:00:11 megatron_gpt_model:1294] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-05-20 21:00:11 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 9648 and consumed_samples: 0\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo I 2024-05-20 21:00:11 nlp_overrides:228] Configuring DDP for model parallelism.\n",
      "[NeMo I 2024-05-20 21:00:11 modelPT:724] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.95]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0006\n",
      "        weight_decay: 0.1\n",
      "    \n",
      "    Parameter Group 1\n",
      "        betas: [0.9, 0.95]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0006\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2024-05-20 21:00:11 lr_scheduler:915] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f07243a5e10>\" \n",
      "    will be used during training (effective maximum steps = 300000) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 750\n",
      "    constant_steps: 80000\n",
      "    min_lr: 6.0e-05\n",
      "    max_steps: 300000\n",
      "    )\n",
      "Sanity Checking: 0it [00:00, ?it/s][NeMo W 2024-05-20 21:00:43 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2024-05-20 21:00:43 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Sanity Checking DataLoader 0: : 65it [00:06, 10.18it/s]                         [NeMo W 2024-05-20 21:00:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2024-05-20 21:01:35 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2024-05-20 21:01:35 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Epoch 0: :   0%|                                            | 0/300000 [00:00<?][NeMo W 2024-05-20 21:01:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2024-05-20 21:01:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('consumed_samples', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2024-05-20 21:01:42 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/megatron/clip_grads.py:86: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "      dummy_overflow_buf = torch.cuda.IntTensor([0])\n",
      "    \n",
      "Epoch 0: :   0%| | 64/300000 [07:55<618:48:14, v_num=2, reduced_train_loss=8.530"
     ]
    }
   ],
   "source": [
    "!python /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py  \\\n",
    "    --config-path=conf/ \\\n",
    "    --config-name=megatron_gpt_config \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.max_epochs=null \\\n",
    "    trainer.max_steps=300000 \\\n",
    "    trainer.val_check_interval=300 \\\n",
    "    trainer.log_every_n_steps=50 \\\n",
    "    trainer.limit_val_batches=50 \\\n",
    "    trainer.limit_test_batches=50 \\\n",
    "    trainer.accumulate_grad_batches=1 \\\n",
    "    trainer.precision=16 \\\n",
    "    model.micro_batch_size=6 \\\n",
    "    model.global_batch_size=192 \\\n",
    "    model.tensor_model_parallel_size=1 \\\n",
    "    model.pipeline_model_parallel_size=1 \\\n",
    "    model.max_position_embeddings=1024 \\\n",
    "    model.encoder_seq_length=1024 \\\n",
    "    model.hidden_size=768 \\\n",
    "    model.ffn_hidden_size=3072 \\\n",
    "    model.num_layers=12 \\\n",
    "    model.num_attention_heads=12 \\\n",
    "    model.init_method_std=0.021 \\\n",
    "    model.hidden_dropout=0.1 \\\n",
    "    model.layernorm_epsilon=1e-5 \\\n",
    "    model.tokenizer.vocab_file=gpt2-vocab.json \\\n",
    "    model.tokenizer.merge_file=gpt2-merges.txt \\\n",
    "    model.data.data_prefix=[1.0,hfbpe_gpt_training_data_context_document] \\\n",
    "    model.data.num_workers=2 \\\n",
    "    model.data.seq_length=1024 \\\n",
    "    model.data.splits_string=\\'980,10,10\\' \\\n",
    "    model.optim.name=fused_adam \\\n",
    "    model.optim.lr=6e-4 \\\n",
    "    model.optim.betas=[0.9,0.95] \\\n",
    "    model.optim.weight_decay=0.1 \\\n",
    "    model.optim.sched.name=CosineAnnealing \\\n",
    "    model.optim.sched.warmup_steps=750 \\\n",
    "    model.optim.sched.constant_steps=80000 \\\n",
    "    model.optim.sched.min_lr=6e-5 \\\n",
    "    exp_manager.resume_if_exists=True \\\n",
    "    exp_manager.resume_ignore_no_checkpoint=True \\\n",
    "    exp_manager.create_checkpoint_callback=True \\\n",
    "    exp_manager.checkpoint_callback_params.monitor=val_loss \\\n",
    "    exp_manager.checkpoint_callback_params.save_top_k=3 \\\n",
    "    exp_manager.checkpoint_callback_params.mode=min \\\n",
    "    exp_manager.checkpoint_callback_params.always_save_nemo=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1737163",
   "metadata": {},
   "source": [
    "Show loss transition in plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcd4084e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"loss.txt\")\n",
    "plt.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b60cf",
   "metadata": {},
   "source": [
    "## Generate text with fine-tuned model\n",
    "\n",
    "Again we check results with our test dataset (5 rows).<br>\n",
    "As you can see below, it can output the completion very well, because it's fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e25008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
